{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a781776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import re \n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import PorterStemmer \n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a76d4",
   "metadata": {},
   "source": [
    "For our web application, in order for the model to understand the text, we must perform text cleaning, preprocessing, and transformation on the user input. The most efficient way to do this, is to build preprocessing pipeline. The things we must do is: \n",
    "* Regex Preprocessing\n",
    "* Tokenization \n",
    "* Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e12befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt=nltk.WordPunctTokenizer()\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9f2dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    doc=re.sub(r'[^a-zA-Z\\s]', '', doc) \n",
    "    doc=doc.lower() \n",
    "    doc=doc.strip() \n",
    "    tokens=wpt.tokenize(doc)\n",
    "    filtered_tokens=[token for token in tokens if token not in stop_words]\n",
    "    doc=' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus=np.vectorize(normalize_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27b0861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ['ginger escobedo needs access legal splash page'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56b7e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new clean text to dataframe\n",
    "normalize_corpus=np.vectorize(normalize_doc) #create a vectorized object for our normalization pipeline\n",
    "norm_text=normalize_corpus(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6ddbf414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ginger escobedo needs access legal splash page'], dtype='<U46')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47adf49",
   "metadata": {},
   "source": [
    "## Final Step - Text Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ed16bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('final_model.pickle','rb') as f:#rb, read-byte\n",
    "    final_model=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "639e4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=final_model.named_steps['tfidf']\n",
    "clf=final_model.named_steps['clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "22ed87ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(max_df=0.75, max_features=10000, min_df=2, ngram_range=(1, 2),\n",
       "                 stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                             'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                             \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                             'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                             'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                             'itself', ...]),\n",
       " LogisticRegression(C=10, class_weight='balanced', random_state=42,\n",
       "                    solver='liblinear'))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f567476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "trans_text=vect.transform(norm_text).toarray()\n",
    "print(clf.predict(trans_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
